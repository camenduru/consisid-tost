{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.5.0+cu124 torchvision==0.20.0+cu124 torchaudio==2.5.0+cu124 torchtext==0.18.0 torchdata==0.8.0 --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install xformers==0.0.28.post2\n",
    "\n",
    "%cd /content\n",
    "!git clone https://github.com/PKU-YuanGroup/ConsisID\n",
    "%cd /content/ConsisID\n",
    "\n",
    "!apt install aria2 -qqy\n",
    "!pip install diffusers transformers accelerate moviepy insightface onnxruntime onnxruntime-gpu facexlib spandrel scikit-video timm ftfy SentencePiece\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/parsing_parsenet.pth -d /content/ConsisID-preview/face_encoder -o parsing_parsenet.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/parsing_bisenet.pth -d /content/ConsisID-preview/face_encoder -o parsing_bisenet.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/detection_Resnet50_Final.pth -d /content/ConsisID-preview/face_encoder -o detection_Resnet50_Final.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/EVA02_CLIP_L_336_psz14_s6B.pt -d /content/ConsisID-preview/face_encoder -o EVA02_CLIP_L_336_psz14_s6B.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/antelopev2/1k3d68.onnx -d /content/ConsisID-preview/face_encoder/models/antelopev2 -o 1k3d68.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/antelopev2/2d106det.onnx -d /content/ConsisID-preview/face_encoder/models/antelopev2 -o 2d106det.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/antelopev2/genderage.onnx -d /content/ConsisID-preview/face_encoder/models/antelopev2 -o genderage.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/antelopev2/glintr100.onnx -d /content/ConsisID-preview/face_encoder/models/antelopev2 -o glintr100.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/antelopev2/scrfd_10g_bnkps.onnx -d /content/ConsisID-preview/face_encoder/models/antelopev2 -o scrfd_10g_bnkps.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/buffalo_l/1k3d68.onnx -d /content/ConsisID-preview/face_encoder/models/buffalo_l -o 1k3d68.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/buffalo_l/2d106det.onnx -d /content/ConsisID-preview/face_encoder/models/buffalo_l -o 2d106det.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/buffalo_l/det_10g.onnx -d /content/ConsisID-preview/face_encoder/models/buffalo_l -o det_10g.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/buffalo_l/genderage.onnx -d /content/ConsisID-preview/face_encoder/models/buffalo_l -o genderage.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/face_encoder/models/buffalo_l/w600k_r50.onnx -d /content/ConsisID-preview/face_encoder/models/buffalo_l -o w600k_r50.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/scheduler/scheduler_config.json -d /content/ConsisID-preview/scheduler -o scheduler_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/text_encoder/config.json -d /content/ConsisID-preview/text_encoder -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/text_encoder/model-00001-of-00002.safetensors -d /content/ConsisID-preview/text_encoder -o model-00001-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/text_encoder/model-00002-of-00002.safetensors -d /content/ConsisID-preview/text_encoder -o model-00002-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/text_encoder/model.safetensors.index.json -d /content/ConsisID-preview/text_encoder -o model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/tokenizer/added_tokens.json -d /content/ConsisID-preview/tokenizer -o added_tokens.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/tokenizer/special_tokens_map.json -d /content/ConsisID-preview/tokenizer -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/tokenizer/spiece.model -d /content/ConsisID-preview/tokenizer -o spiece.model\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/tokenizer/tokenizer_config.json -d /content/ConsisID-preview/tokenizer -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/transformer/config.json -d /content/ConsisID-preview/transformer -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/transformer/diffusion_pytorch_model-00001-of-00002.safetensors -d /content/ConsisID-preview/transformer -o diffusion_pytorch_model-00001-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/transformer/diffusion_pytorch_model-00002-of-00002.safetensors -d /content/ConsisID-preview/transformer -o diffusion_pytorch_model-00002-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/transformer/diffusion_pytorch_model.safetensors.index.json -d /content/ConsisID-preview/transformer -o diffusion_pytorch_model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/vae/config.json -d /content/ConsisID-preview/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/resolve/main/vae/diffusion_pytorch_model.safetensors -d /content/ConsisID-preview/vae -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/configuration.json -d /content/ConsisID-preview -o configuration.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/BestWishYsh/ConsisID-preview/raw/main/model_index.json -d /content/ConsisID-preview -o model_index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ai-forever/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -d /content/model_real_esran -o RealESRGAN_x4.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/AlexWortega/RIFE/resolve/main/flownet.pkl -d /content/model_rife -o flownet.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ConsisID\n",
    "\n",
    "import os, shutil, random, time, requests\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from datetime import datetime\n",
    "\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from facexlib.parsing import init_parsing_model\n",
    "from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n",
    "\n",
    "from diffusers import CogVideoXDPMScheduler\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.training_utils import free_memory\n",
    "\n",
    "from util.utils import *\n",
    "from util.rife_model import load_rife_model\n",
    "from models.utils import process_face_embeddings\n",
    "from models.transformer_consisid import ConsisIDTransformer3DModel\n",
    "from models.pipeline_consisid import ConsisIDPipeline\n",
    "from models.eva_clip import create_model_and_transforms\n",
    "from models.eva_clip.constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n",
    "from models.eva_clip.utils_qformer import resize_numpy_image_long\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"BestWishYsh/ConsisID-preview\"\n",
    "lora_path = None\n",
    "lora_rank = 128\n",
    "dtype = torch.bfloat16\n",
    "if os.path.exists(os.path.join(model_path, \"transformer_ema\")):\n",
    "    subfolder = \"transformer_ema\"\n",
    "else:\n",
    "    subfolder = \"transformer\"        \n",
    "transformer = ConsisIDTransformer3DModel.from_pretrained_cus(model_path, subfolder=subfolder)\n",
    "scheduler = CogVideoXDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "try:\n",
    "    is_kps = transformer.config.is_kps\n",
    "except:\n",
    "    is_kps = False\n",
    "face_helper = FaceRestoreHelper(\n",
    "    upscale_factor=1,\n",
    "    face_size=512,\n",
    "    crop_ratio=(1, 1),\n",
    "    det_model='retinaface_resnet50',\n",
    "    save_ext='png',\n",
    "    device=device,\n",
    "    model_rootpath=os.path.join(model_path, \"face_encoder\")\n",
    ")\n",
    "face_helper.face_parse = None\n",
    "face_helper.face_parse = init_parsing_model(model_name='bisenet', device=device, model_rootpath=os.path.join(model_path, \"face_encoder\"))\n",
    "face_helper.face_det.eval()\n",
    "face_helper.face_parse.eval()\n",
    "model, _, _ = create_model_and_transforms('EVA02-CLIP-L-14-336', os.path.join(model_path, \"face_encoder\", \"EVA02_CLIP_L_336_psz14_s6B.pt\"), force_custom_clip=True)\n",
    "face_clip_model = model.visual\n",
    "face_clip_model.eval()\n",
    "eva_transform_mean = getattr(face_clip_model, 'image_mean', OPENAI_DATASET_MEAN)\n",
    "eva_transform_std = getattr(face_clip_model, 'image_std', OPENAI_DATASET_STD)\n",
    "if not isinstance(eva_transform_mean, (list, tuple)):\n",
    "    eva_transform_mean = (eva_transform_mean,) * 3\n",
    "if not isinstance(eva_transform_std, (list, tuple)):\n",
    "    eva_transform_std = (eva_transform_std,) * 3\n",
    "eva_transform_mean = eva_transform_mean\n",
    "eva_transform_std = eva_transform_std\n",
    "face_main_model = FaceAnalysis(name='antelopev2', root=os.path.join(model_path, \"face_encoder\"), providers=['CUDAExecutionProvider'])\n",
    "handler_ante = insightface.model_zoo.get_model(f'{model_path}/face_encoder/models/antelopev2/glintr100.onnx', providers=['CUDAExecutionProvider'])\n",
    "face_main_model.prepare(ctx_id=0, det_size=(640, 640))\n",
    "handler_ante.prepare(ctx_id=0)\n",
    "face_clip_model.to(device, dtype=dtype)\n",
    "face_helper.face_det.to(device)\n",
    "face_helper.face_parse.to(device)\n",
    "transformer.to(device, dtype=dtype)\n",
    "free_memory()\n",
    "pipe = ConsisIDPipeline.from_pretrained(model_path, transformer=transformer, scheduler=scheduler, torch_dtype=dtype)\n",
    "if lora_path:\n",
    "    pipe.load_lora_weights(lora_path, weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"test_1\")\n",
    "    pipe.fuse_lora(lora_scale=1 / lora_rank)\n",
    "scheduler_args = {}\n",
    "if \"variance_type\" in pipe.scheduler.config:\n",
    "    variance_type = pipe.scheduler.config.variance_type\n",
    "    if variance_type in [\"learned\", \"learned_range\"]:\n",
    "        variance_type = \"fixed_small\"\n",
    "    scheduler_args[\"variance_type\"] = variance_type\n",
    "pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, **scheduler_args)\n",
    "pipe.to(device)\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "pipe.vae.enable_slicing()\n",
    "pipe.vae.enable_tiling()\n",
    "upscale_model = load_sd_upscale(\"/content/model_real_esran/RealESRGAN_x4.pth\", device)\n",
    "frame_interpolation_model = load_rife_model(\"/content/model_rife\")\n",
    "\n",
    "def download_file(url, save_dir, file_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_suffix = os.path.splitext(urlsplit(url).path)[1]\n",
    "    file_name_with_suffix = file_name + file_suffix\n",
    "    file_path = os.path.join(save_dir, file_name_with_suffix)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(input):\n",
    "    values = input[\"input\"]\n",
    "\n",
    "    input_image = values['input_image']\n",
    "    input_image = download_file(url=input_image, save_dir='/content', file_name='input_image')\n",
    "    prompt = values['prompt']\n",
    "    num_inference_steps = values['num_inference_steps']\n",
    "    guidance_scale = values['guidance_scale']\n",
    "    seed = values['seed']\n",
    "\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    print(seed)\n",
    "\n",
    "    image_input = Image.open(input_image).convert(\"RGB\")\n",
    "    id_image = np.array(ImageOps.exif_transpose(image_input))\n",
    "    id_image = resize_numpy_image_long(id_image, 1024)\n",
    "    id_cond, id_vit_hidden, align_crop_face_image, face_kps = process_face_embeddings(face_helper, face_clip_model, handler_ante, \n",
    "                                                                            eva_transform_mean, eva_transform_std, \n",
    "                                                                            face_main_model, device, dtype, id_image, \n",
    "                                                                            original_id_image=id_image, is_align_face=True, \n",
    "                                                                            cal_uncond=False)\n",
    "    kps_cond = face_kps if is_kps else None\n",
    "    prompt = prompt.strip('\"')\n",
    "    generator = torch.Generator(device).manual_seed(seed) if seed else None\n",
    "    latents = pipe(\n",
    "        prompt=prompt,\n",
    "        image=image_input,\n",
    "        num_videos_per_prompt=1,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_frames=49,\n",
    "        use_dynamic_cfg=False,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "        id_vit_hidden=id_vit_hidden,\n",
    "        id_cond=id_cond,\n",
    "        kps_cond=kps_cond,\n",
    "        output_type=\"pt\",\n",
    "    ).frames\n",
    "    batch_size = latents.shape[0]\n",
    "    batch_video_frames = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        pt_image = latents[batch_idx]\n",
    "        pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
    "        image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
    "        image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
    "        batch_video_frames.append(image_pil)\n",
    "    all_frames = [frame for sublist in batch_video_frames for frame in sublist]\n",
    "    total_frames = len(all_frames)\n",
    "    desired_duration_seconds = 6\n",
    "    fps = math.ceil(total_frames / desired_duration_seconds)\n",
    "    video_path = save_video(all_frames, fps)\n",
    "    free_memory()\n",
    "    source = video_path\n",
    "    destination = '/content/consisid-tost.mp4'\n",
    "    shutil.move(source, destination)\n",
    "\n",
    "    return f\"/content/consisid-tost.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = { \n",
    "        \"input\": {\n",
    "            \"input_image\": \"https://files.catbox.moe/wy4gq7.jpg\",\n",
    "            \"prompt\": \"A woman adorned with a delicate flower crown, is standing amidst a field of gently swaying wildflowers. Her eyes sparkle with a serene gaze, and a faint smile graces her lips, suggesting a moment of peaceful contentment. The shot is framed from the waist up, highlighting the gentle breeze lightly tousling her hair. The background reveals an expansive meadow under a bright blue sky, capturing the tranquility of a sunny afternoon.\",\n",
    "            \"num_inference_steps\": 50,\n",
    "            \"guidance_scale\": 7.0,\n",
    "            \"seed\": 42\n",
    "        }\n",
    "}\n",
    "video = generate(input)\n",
    "from IPython.display import Video\n",
    "Video(video, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConsisID-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
